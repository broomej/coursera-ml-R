---
title: "Exercise 2"
author: "Jai Broome"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
    html_document:
        toc: true
        number_sections: true
---

```{r dependencies, message=FALSE}
require(ggplot2)
require(knitr)
```

```{r read_chunks}
read_chunk("ex2_chunks.R")
```

# Logistic Regression

## Visualizing the Data

Read in data and initialize theta

```{r read-data}
ex2data1 <- read.table("../data/ex2data1.txt", sep = ",")
ex2data1 <- cbind(1, ex2data1)
initial_theta <- rep(0, times = 3)
```

```{r vizualize-data}
g1 <- ggplot(ex2data1, aes(x = V1, y = V2, color = as.factor(V3))) + 
    geom_point() +
    labs(x = "Exam 1 Score",
         y = "Exam 2 Score",
         color = "Legend\n") +
    scale_color_manual(labels = c("Not Admitted", "Admitted"), values = c(1,2))
g1 + ggtitle("Figure 1: Scatter plot of training data")
```


## Implementation

### Sigmoid function

```{r sig}
```

### Cost function and gradient

Note that `costFunction` takes a matrix `M` with the outcome in the last column, and separates it into Xs and Y. Note that I've implemented regularization for the first exercise so the function can be reused for the second exercise.

```{r cost-function}
```

### Learning parameters using `optim`

I used R's `optim` as an alternative to Matlab's `fminunc`. `costFunction` returns both the cost and the gradient.

```{r learn-theta}
ex2data1 <- as.matrix(ex2data1)
newTheta <- optim(par = initial_theta,
      fn = function(x){costFunction(ex2data1, x)$J},
      gr = function(x){costFunction(ex2data1, x)$grad},
      method = "BFGS", control = list(maxit = 400))
```

### Evaluating Logistic Regression

```{r test-scores}
testScores <- c(45, 85)
```

With test scores of `r testScores`, we should see an admission probability of 0.776.

```{r h}
```


```{r numeric-test}
h(newTheta$par, c(1, testScores)) # Include the x0 term
```

While the in-sample error isn't reported in the excercise sheet, the above test makes me reasonably sure that this is working correctly. I'll update this if I hear from anyone who has taken this course for credit.

```{r pred-log-reg}
```

```{r train-accuracy}
ex2data1Pred <- predLogReg(ex2data1, newTheta$par)
sum(ex2data1[, 4]==round(ex2data1Pred, 0)) / nrow(ex2data1)
```

By eyeballing the plot in the exercise sheet, we can see the decision boundary lies about on (50, 75) and (75, 50). By using those points as initial parameters, I find two points on the decision boundary using `optim`.

```{r find-decision-boundary}
```



```{r calc-decision-boundary}
decPoints <- findDecisionBoundary(theta = newTheta$par)
```

The prediction should be 0.5 for both points (definitionally), so we can see if we made a mistake

```{r test-dec-points}
apply(decPoints[, c(1, 3)], 1, function(x){h(c(1, x), newTheta$par)})
```

```{r plot-dec-boundary}
dec.lm <- lm(x ~ newY, decPoints)
g2 <- g1 +
    geom_abline(slope = dec.lm$coefficients[[2]], intercept = dec.lm$coefficients[[1]])
g2 + ggtitle("Figure 2: Training data with decision boundary")
```


# Regularized Logistic Regression

## Visualizing the data

```{r}
ex2data2 <- read.table("../data/ex2data2.txt", sep = ",")
ggplot(ex2data2, aes(V1, V2)) + geom_point(aes(shape = as.factor(V3), color = as.factor(V3)))
```

## Feature mapping

```{r}
y <- ex2data2$V3

x1s <- ex2data2[, 1]
x2s <- ex2data2[, 2]

for(i in 2:6){
    x1s <- cbind(x1s, ex2data2[,1] ^ i)
    }
x1s <- cbind(1, x1s)

for(i in 2:6){
    x2s <- cbind(x2s, ex2data2[,1] ^ i)
}
x2s <- cbind(1, x2s)

allxs <- vector()

## We only want up to 6 degree polynomials, so this gives us too many
for(i in 1:ncol(x2s)){
    allxs <- cbind(allxs, x1s[, 1:(8-i)] * x2s[,i])
}

ex2data2.full <- cbind(allxs, y)
```

## Cost function and gradient

```{r}
initial_theta <- rep(0, times = 28)

costFunction(ex2data2.full, initial_theta)
```

## Plotting the decision boundary

## Optional (ungraded) exercises

