{
    "contents" : "require(R.matlab)\nex4data1 <- readMat(\"ex4data1.mat\")\nex4data1$X <- cbind(1, ex4data1$X)\n\nsig <- function(x){\n  1 / (1 + exp(-x))\n}\n\nnewy <- vector()\nfor(i in 1:10){\n  newy <- cbind(newy, ex4data1$y == i)\n}\n\nex4weights <- readMat(\"ex4weights.mat\")\n\n# from kaleko on Github\n# https://github.com/kaleko/CourseraML/blob/master/ex4/ex4.ipynb\n\ninput_layer_size <- 400\nhidden_layer_size <- 25\noutput_layer_size <- 10 \nn_training_samples <- 5000\n\n# unlist() does what flattenParams does in kaleko's python script\n\n# flattened_array <- unlist(ex4weights)\nreshapeParams <- function(flattened_array){\n  theta1 <- matrix(flattened_array[1:((input_layer_size+1)*hidden_layer_size)],\n                   nrow = hidden_layer_size,\n                   ncol = input_layer_size + 1,\n                   byrow = FALSE)\n  \n  theta2 <- matrix(flattened_array[((input_layer_size+1)*hidden_layer_size + 1):\n                                     length(flattened_array)],\n                   nrow = output_layer_size,\n                   ncol = hidden_layer_size + 1,\n                   byrow = FALSE)\n  \n  return(list(theta1 = theta1, theta2 = theta2))\n}\n\n# flattenedX <- unlist(ex4data1$X)\nreshapeX <- function(flattenedX){\n  xReshaped <- matrix(flattenedX,\n                     nrow = n_training_samples,\n                     ncol = (input_layer_size+1),\n                     byrow = FALSE)\n  return(xReshaped)\n}\n\n# sum(ex4data1$X!=xReshaped)\n# \n# mythetas_flattened <- unlist(ex4weights)\n# myX_flattened <- unlist(ex4data1$X)\n# myy <- newy\n# mylambda = 0\ncomputeCost <- function(mythetas_flattened, myX_flattened, myy, mylambda = 0){\n  # Modified to take (m X k) dimensional y matrix\n  \n  # First unroll the parameters\n  mythetas <- reshapeParams(mythetas_flattened)\n  \n  # Now unroll X\n  myX <- reshapeX(myX_flattened)\n  \n  #This is what will accumulate the total cost\n  total_cost <- 0\n  \n  m <- n_training_samples\n  \n  # irow <- 100\n  for(irow in 1:m){\n    myrow <- myX[irow, ]\n    myhs <- propagateForward(myrow,mythetas)[[2]][,2] \n    tmpy <- myy[irow, ]\n    mycost <- - crossprod(c(tmpy, 1 - tmpy), c(log(myhs), log(1 - myhs))) \n    total_cost <- total_cost + mycost\n  }\n  total_cost <- total_cost / m\n  \n  total_reg <- 0\n  for(mytheta in mythetas){\n    total_reg <- total_reg + sum(mytheta * mytheta)\n  }\n  total_reg <- total_reg * mylambda / (2 * m)\n  return(total_cost + total_reg)\n}\n\n# row <- ex4data1$X[1, ]\n# Thetas <- ex4weights\npropagateForward <- function(row, Thetas){\n  features <- row\n  zs_as_per_layer <- list()\n  \n  for(i in 1:length(Thetas)){\n  # i <- 1\n    Theta <- Thetas[[i]] \n    \n    #Theta1 is (25,401), features are (401, 1)\n    #so \"z\" comes out to be (25, 1)\n    #this is one \"z\" value for each unit in the hidden layer\n    #not counting the bias unit\n    z <- Theta %*% features\n    a <- sig(z)\n    zs_as_per_layer[[i]] <- cbind(z, a) \n    if(i == length(Thetas)) {\n      return(zs_as_per_layer)\n    }\n    a <- c(1, a)\n    features <- a\n    # i <- 2\n  }\n}\n# propagateForward(ex4data1$X[1, ], Thetas <- ex4weights)\n\ncomputeCost(unlist(ex4weights), unlist(ex4data1$X), newy)\ncomputeCost(unlist(ex4weights), unlist(ex4data1$X), newy, 1)\n\nsigmoidGradient <- function(z){\n  sig(z) * (1 - sig(z))\n}\n\ngenRandThetas <- function(epsilon_init = 0.12){\n  t1 <- matrix(runif(hidden_layer_size * (input_layer_size+1), -1),\n               hidden_layer_size,\n               input_layer_size + 1)\n  t2 <- matrix(runif(output_layer_size * (hidden_layer_size+1)),\n               output_layer_size,\n               hidden_layer_size+1)\n  return(list(Theta1 = t1 * epsilon_init, Theta2 = t2 * epsilon_init))\n}\n\nbackPropagate <- function(mythetas_flattened, myX_flattened, myy, mylambda = 0){\n  # First unroll the parameters\n  mythetas <- reshapeParams(mythetas_flattened)\n  \n  # Now unroll X\n  myX <- reshapeX(myX_flattened)\n  \n  Delta1 = matrix(0, hidden_layer_size,input_layer_size+1)\n  Delta2 = matrix(0, output_layer_size,hidden_layer_size+1)\n  \n  m = n_training_samples\n  # irow <- 1\n  for(irow in 1:m){\n    myrow <- myX[irow,]\n    a1 <- myrow\n    # propagateForward returns (zs, activations) for each layer excluding the input layer\n    temp = propagateForward(myrow,mythetas)\n    z2 = temp[[1]][,1]\n    a2 = temp[[1]][,2]\n    z3 = temp[[2]][,1]\n    a3 = temp[[2]][,2]\n    tmpy <- myy[irow, ]\n    delta3 = a3 - tmpy \n    \n    delta2 <- (t(mythetas[[2]])[-1,] %*% delta3) * sigmoidGradient(z2)\n    a2 <- c(1,a2)\n    Delta1 <- Delta1 + delta2 %*% t(a1)\n    Delta2 <- Delta2 + delta3 %*% t(a2)\n  }\n  D1 <- Delta1 / m\n  D2 <- Delta2 / m\n  \n  #Regularization:\n  D1[, -1] <- D1[, -1] + (mylambda / m) * mythetas[[1]][,-1]\n  D2[, -1] <- D2[, -1] + (mylambda / m) * mythetas[[2]][,-1]\n  \n  return(unlist(list(D1, D2)))\n}\n\n\n#Actually compute D matrices for the Thetas provided\nflattenedD1D2 = backPropagate(unlist(ex4weights),\n                              unlist(ex4data1$X),\n                              ex4data1$y,\n                              mylambda = 0)\n\ndeltas <- reshapeParams(flattenedD1D2)\n\ncheckGradient <- function(mythetas,myDs,myX,myy,mylambda=0){\n  myeps <- 0.0001\n  flattened <- unlist(mythetas)\n  flattenedDs <- unlist(myDs)\n  myX_flattened <- unlist(myX)\n  n_elems <- length(flattened)\n  # Pick ten random elements, compute numerical gradient, compare to respective D's\n  for(i in 1:10){\n    x <- as.integer(runif(1) * n_elems)\n    epsvec <- rep(0, times = n_elems)\n    epsvec[x] <- myeps\n    cost_high <- computeCost(flattened + epsvec,myX_flattened,myy,mylambda)\n    cost_low <- computeCost(flattened - epsvec,myX_flattened,myy,mylambda)\n    mygrad <- (cost_high - cost_low) / (2*myeps)\n    return(list(element = x, num.grad = mygrad, backprop.grad = flattenedDs[x]))\n  }\n}\n\ncheckGradient(ex4weights, deltas, ex4data1$X, newy)\n\n## Learn parameters with optim()\n\ntrainNN <- function(mylambda=0, it = 50){\n  randomThetas_unrolled <- unlist(genRandThetas())\n  result <- optim(par = randomThetas_unrolled,\n                  fn = function(x){computeCost(x, unlist(ex4data1$X), newy)},\n                  gr = function(x)(backPropagate(x, unlist(ex4data1$X), newy)),\n                  method = \"BFGS\", control = list(maxit = it))\n}\n\nlearned_Thetas <- trainNN()\n\npredictNN <- function(row,Thetas){\n  \n}\n\nNNpred <- function(myX,myThetas,myy){ #takes vector of ys\n  apply(myX, 1, function(x){\n    which.max(propagateForward(x, myThetas)[[2]][,2]) \n    })\n}\n\npred <- NNpred(ex4data1$X, reshapeParams(learned_Thetas$par), ex4data1$y)\n\nsum(pred==ex4data1$y) / length(pred)",
    "created" : 1463709877460.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4098650052",
    "id" : "4AB82D3D",
    "lastKnownWriteTime" : 1463710383,
    "path" : "~/Dropbox/ex4v3/ex4frompy.R",
    "project_path" : "ex4frompy.R",
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "type" : "r_source"
}