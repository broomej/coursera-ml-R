}
## @knitr random-learningCurve
## just randomize train, then call learningCurve
## @knitr lin-reg-cost-func
computeCost <- function(X, y, theta, lambda = 0){
if(is.null(dim(X))){
X <- t(data.frame(X))
}
m <- nrow(X)
pred <- theta %*% t(X)
sqError <- (pred - y) ^ 2
cost <- (1 / (2 * m)) * sum(sqError)
reg <- (lambda / (2 * m)) * sum(theta[-1]^2)
J <- cost + reg
gradient <- (1 / m) * sum((pred - y) * X[, 1]) # first element unregularized
for(j in 2:length(theta)){
gradient <- c(gradient, ((1 / m) * sum((pred - y) * X[, j]) + lambda * theta[j] / m))
}
return(list(J=J, gradient=gradient))
}
## @knitr gradStep
gradStep <- function(thetaj, alpha, gradj){
thetaj - alpha * gradj
}
## @knitr gradientDescent
gradientDescent <- function(X, y, theta, alpha, iterations, lambda = 0){
if(length(theta) != ncol(X)){stop("theta and X are nonconformable")}
J_history <- data.frame()
for (iteration in 1:iterations){
a <- computeCost(X = X, y = y, theta = theta, lambda = lambda)
J <- a$J
gradient <- a$gradient
J_history <- rbind(J_history, c(J, iteration - 1, theta))
thetaTemp <- vector()
for(j in 1:length(theta)){
thetaTemp <- c(thetaTemp, gradStep(theta[j], alpha, gradient[j]))
}
theta <- thetaTemp
}
# for final iteration
J <- computeCost(X = X, y = y, theta = theta, lambda = lambda)$J
J_history <- rbind(J_history, c(J, iteration, theta))
thetanames <- rep("theta", times = length(theta) - 1)
for(i in length(thetanames)){thetanames[i] <- paste("theta", i, sep = "")}
colnames(J_history) <- c("loss", "iterations", "theta0", thetanames)
return(J_history)
}
## @knitr featureNormalize
featureNormalize <- function(X){
apply(X, 2, FUN = function(x){(x - mean(x)) / sd(x)})
}
source('~/coursera-ml-R/ex1/ex1_chunks.R')
source('~/coursera-ml-R/ex5/ex5_chunks.R')
knitr::read_chunk('../ex1/ex1_chunks.R')
g1 <- ggplot(data = train, aes(X, y)) +
geom_point(shape = 4, color = "red", size = 3) +
labs(title = "Figure 1: Training Data",
x = "Change in water level (X)",
y = "Water flowing out of the dam (y)")
g1
learnedTheta <- trainLinearReg(cbind(1, train$X), train$y)
polyPlots(train$X, train$y, val$X, val$y, n = 12, lambda = 0, p = 8)
lseq <- c(0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10)
errors <- validationCurve(polyFeatures(train$X, p = 8),
train$y,
polyFeatures(val$X, p = 8),
val$y,
lseq)
errors[, 1] <- 1:10 # for interpretability in the plot
plotLearningCurve(errors)
require(ggplot2)
computeCost(cbind(1, train$X), train$y, c(1,1))
train <- read.csv("../data/ex5train.csv")
val <- read.csv("../data/ex5val.csv")
test <- read.csv("../data/ex5test.csv")
knitr::read_chunk('ex5_chunks.R')
errors <- learningCurve(cbind(1, train$X), train$y, cbind(1, val$X), val$y)
g2 <- plotLearningCurve(errors)
g2
g1 + geom_abline(slope = learnedTheta$par[2], intercept = learnedTheta$par[1])
lm(y ~ X + 1, train)$coefficients
learnedTheta$par
# Chunk 1: dependencies
require(ggplot2)
# Chunk 2: read-data
train <- read.csv("../data/ex5train.csv")
val <- read.csv("../data/ex5val.csv")
test <- read.csv("../data/ex5test.csv")
# Chunk 3: viz-train
g1 <- ggplot(data = train, aes(X, y)) +
geom_point(shape = 4, color = "red", size = 3) +
labs(title = "Figure 1: Training Data",
x = "Change in water level (X)",
y = "Water flowing out of the dam (y)")
g1
# Chunk 4: ex1_chunks
knitr::read_chunk('../ex1/ex1_chunks.R')
# Chunk 5: lin-reg-cost-func
# Chunk 6: gradient-check
computeCost(cbind(1, train$X), train$y, c(1,1))
# Chunk 7: ex5_chunks
knitr::read_chunk('ex5_chunks.R')
# Chunk 8: trainLinearReg
# Chunk 9: optim
learnedTheta <- trainLinearReg(cbind(1, train$X), train$y)
# Chunk 10: plot-learnedTheta
g1 + geom_abline(slope = learnedTheta$par[2], intercept = learnedTheta$par[1])
lm(y ~ X + 1, train)$coefficients
learnedTheta$par
# Chunk 11: learningCurve
# Chunk 12: plot-learning-curves
errors <- learningCurve(cbind(1, train$X), train$y, cbind(1, val$X), val$y)
g2 <- plotLearningCurve(errors)
g2
# Chunk 13: polyFeatures
# Chunk 14: featureNormalize
# Chunk 15: polyPlots
# Chunk 16
polyPlots(train$X, train$y, val$X, val$y, n = 12, lambda = 0, p = 8)
# Chunk 17: validationCurve
# Chunk 18: test-validationCurve
lseq <- c(0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10)
errors <- validationCurve(polyFeatures(train$X, p = 8),
train$y,
polyFeatures(val$X, p = 8),
val$y,
lseq)
errors[, 1] <- 1:10 # for interpretability in the plot
plotLearningCurve(errors)
setwd("ex5")
# Chunk 1: dependencies
require(ggplot2)
# Chunk 2: read-data
train <- read.csv("../data/ex5train.csv")
val <- read.csv("../data/ex5val.csv")
test <- read.csv("../data/ex5test.csv")
# Chunk 3: viz-train
g1 <- ggplot(data = train, aes(X, y)) +
geom_point(shape = 4, color = "red", size = 3) +
labs(title = "Figure 1: Training Data",
x = "Change in water level (X)",
y = "Water flowing out of the dam (y)")
g1
# Chunk 4: ex1_chunks
knitr::read_chunk('../ex1/ex1_chunks.R')
# Chunk 5: lin-reg-cost-func
# Chunk 6: gradient-check
computeCost(cbind(1, train$X), train$y, c(1,1))
# Chunk 7: ex5_chunks
knitr::read_chunk('ex5_chunks.R')
# Chunk 8: trainLinearReg
# Chunk 9: optim
learnedTheta <- trainLinearReg(cbind(1, train$X), train$y)
# Chunk 10: plot-learnedTheta
g1 + geom_abline(slope = learnedTheta$par[2], intercept = learnedTheta$par[1])
lm(y ~ X + 1, train)$coefficients
learnedTheta$par
# Chunk 11: learningCurve
# Chunk 12: plot-learning-curves
errors <- learningCurve(cbind(1, train$X), train$y, cbind(1, val$X), val$y)
g2 <- plotLearningCurve(errors)
g2
# Chunk 13: polyFeatures
# Chunk 14: featureNormalize
# Chunk 15: polyPlots
# Chunk 16
polyPlots(train$X, train$y, val$X, val$y, n = 12, lambda = 0, p = 8)
# Chunk 17: validationCurve
# Chunk 18: test-validationCurve
lseq <- c(0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10)
errors <- validationCurve(polyFeatures(train$X, p = 8),
train$y,
polyFeatures(val$X, p = 8),
val$y,
lseq)
errors[, 1] <- 1:10 # for interpretability in the plot
plotLearningCurve(errors)
source('~/coursera-ml-R/ex5/ex5_chunks.R')
source('~/coursera-ml-R/ex1/ex1_chunks.R')
# Chunk 1: dependencies
require(ggplot2)
# Chunk 2: read-data
train <- read.csv("../data/ex5train.csv")
val <- read.csv("../data/ex5val.csv")
test <- read.csv("../data/ex5test.csv")
# Chunk 3: viz-train
g1 <- ggplot(data = train, aes(X, y)) +
geom_point(shape = 4, color = "red", size = 3) +
labs(title = "Figure 1: Training Data",
x = "Change in water level (X)",
y = "Water flowing out of the dam (y)")
g1
# Chunk 4: ex1_chunks
knitr::read_chunk('../ex1/ex1_chunks.R')
# Chunk 5: lin-reg-cost-func
# Chunk 6: gradient-check
computeCost(cbind(1, train$X), train$y, c(1,1))
# Chunk 7: ex5_chunks
knitr::read_chunk('ex5_chunks.R')
# Chunk 8: trainLinearReg
# Chunk 9: optim
learnedTheta <- trainLinearReg(cbind(1, train$X), train$y)
# Chunk 10: plot-learnedTheta
g1 + geom_abline(slope = learnedTheta$par[2], intercept = learnedTheta$par[1])
lm(y ~ X + 1, train)$coefficients
learnedTheta$par
# Chunk 11: learningCurve
# Chunk 12: plot-learning-curves
errors <- learningCurve(cbind(1, train$X), train$y, cbind(1, val$X), val$y)
g2 <- plotLearningCurve(errors)
g2
# Chunk 13: polyFeatures
# Chunk 14: featureNormalize
# Chunk 15: polyPlots
# Chunk 16
polyPlots(train$X, train$y, val$X, val$y, n = 12, lambda = 0, p = 8)
# Chunk 17: validationCurve
# Chunk 18: test-validationCurve
lseq <- c(0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10)
errors <- validationCurve(polyFeatures(train$X, p = 8),
train$y,
polyFeatures(val$X, p = 8),
val$y,
lseq)
errors[, 1] <- 1:10 # for interpretability in the plot
plotLearningCurve(errors)
install.packages("ggplot2")
install.packages("knitr")
require(ggplot2)
# Chunk 1: dependencies
require(ggplot2)
# Chunk 2: read-data
train <- read.csv("../data/ex5train.csv")
val <- read.csv("../data/ex5val.csv")
test <- read.csv("../data/ex5test.csv")
# Chunk 3: viz-train
g1 <- ggplot(data = train, aes(X, y)) +
geom_point(shape = 4, color = "red", size = 3) +
labs(title = "Figure 1: Training Data",
x = "Change in water level (X)",
y = "Water flowing out of the dam (y)")
g1
# Chunk 4: ex1_chunks
knitr::read_chunk('../ex1/ex1_chunks.R')
# Chunk 5: lin-reg-cost-func
# Chunk 6: gradient-check
computeCost(cbind(1, train$X), train$y, c(1,1))
# Chunk 7: ex5_chunks
knitr::read_chunk('ex5_chunks.R')
# Chunk 8: trainLinearReg
# Chunk 9: optim
learnedTheta <- trainLinearReg(cbind(1, train$X), train$y)
# Chunk 10: plot-learnedTheta
g1 + geom_abline(slope = learnedTheta$par[2], intercept = learnedTheta$par[1])
lm(y ~ X + 1, train)$coefficients
learnedTheta$par
# Chunk 11: learningCurve
# Chunk 12: plot-learning-curves
errors <- learningCurve(cbind(1, train$X), train$y, cbind(1, val$X), val$y)
g2 <- plotLearningCurve(errors)
g2
# Chunk 13: polyFeatures
# Chunk 14: featureNormalize
# Chunk 15: polyPlots
# Chunk 16
polyPlots(train$X, train$y, val$X, val$y, n = 12, lambda = 0, p = 8)
# Chunk 17: validationCurve
# Chunk 18: test-validationCurve
lseq <- c(0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10)
errors <- validationCurve(polyFeatures(train$X, p = 8),
train$y,
polyFeatures(val$X, p = 8),
val$y,
lseq)
errors[, 1] <- 1:10 # for interpretability in the plot
plotLearningCurve(errors)
q1data <- read.table("../data/ex1data1.txt",
sep = ",",
col.names = c("population", "profit"))
q1data$ones <- 1
q1data <- q1data %>% select(ones, population, profit)
X <- select(q1data, -profit)
y <- q1data$profit
m <- length(y)
require(dplyr)
install.packages("dplyr")
require(dplyr)
q1data <- read.table("../data/ex1data1.txt",
sep = ",",
col.names = c("population", "profit"))
q1data$ones <- 1
q1data <- q1data %>% select(ones, population, profit)
X <- select(q1data, -profit)
y <- q1data$profit
m <- length(y)
theta <- rep(0, times = 2)
iterations <- 1500
alpha <- 0.01
pred <- theta %*% t(X)
source("ex1_chunks.R")
debug(computeCost)
temp <- computeCost(X, y, theta, lambda = 0)
jhist <- gradientDescent(X, y, theta, alpha, iterations)
?matrix
computeCost(matrix(c(1, 2, 1, 3, 1, 4, 1, 5), 4, 2, FALSE),
c(7, 6, 5, 4),
c(0.1, 0.2))
undebug(computeCost)
computeCost(matrix(c(1, 2, 1, 3, 1, 4, 1, 5), 4, 2, FALSE),
c(7, 6, 5, 4),
c(0.1, 0.2))
computeCost(matrix(c(1, 2, 1, 3, 1, 4, 1, 5), 4, 2, TRUE),
c(7, 6, 5, 4),
c(0.1, 0.2))
matrix(c(1, 2, 1, 3, 1, 4, 1, 5), 4, 2, TRUE)
?computeCost
source('~/coursera-ml-R/ex1/ex1_chunks.R')
c(0.1, 0.2)
c(7, 6, 5, 4)
matrix(c(1, 2, 1, 3, 1, 4, 1, 5), 4, 2, TRUE)
computeCost(matrix(c(1, 2, 1, 3, 1, 4, 1, 5), 4, 2, TRUE),
c(7, 6, 5, 4),
c(0.1, 0.2))
c(0.1,0.2,0.3)
c(7,6,5,4)
matrix(c(1, 2, 3, 1, 3, 4, 1, 4, 5, 1, 5, 6))
matrix(c(1, 2, 1, 3, 1, 4, 1, 5), 4, 2, TRUE)
matrix(c(1, 2, 3, 1, 3, 4, 1, 4, 5, 1, 5, 6), 4, 3, TRUE)
computeCost(matrix(c(1, 2, 3, 1, 3, 4, 1, 4, 5, 1, 5, 6), 4, 3, TRUE),
c(7,6,5,4),
c(0.1,0.2,0.3))
source('~/coursera-ml-R/ex5/ex5_chunks.R')
source('~/coursera-ml-R/ex1/ex1_chunks.R')
# Chunk 1: dependencies
require(ggplot2)
# Chunk 2: read-data
train <- read.csv("../data/ex5train.csv")
val <- read.csv("../data/ex5val.csv")
test <- read.csv("../data/ex5test.csv")
# Chunk 3: viz-train
g1 <- ggplot(data = train, aes(X, y)) +
geom_point(shape = 4, color = "red", size = 3) +
labs(title = "Figure 1: Training Data",
x = "Change in water level (X)",
y = "Water flowing out of the dam (y)")
g1
# Chunk 4: ex1_chunks
knitr::read_chunk('../ex1/ex1_chunks.R')
# Chunk 5: lin-reg-cost-func
# Chunk 6: gradient-check
computeCost(cbind(1, train$X), train$y, c(1,1))
# Chunk 7: ex5_chunks
knitr::read_chunk('ex5_chunks.R')
# Chunk 8: trainLinearReg
# Chunk 9: optim
learnedTheta <- trainLinearReg(cbind(1, train$X), train$y)
# Chunk 10: plot-learnedTheta
g1 + geom_abline(slope = learnedTheta$par[2], intercept = learnedTheta$par[1])
lm(y ~ X + 1, train)$coefficients
learnedTheta$par
# Chunk 11: learningCurve
# Chunk 12: plot-learning-curves
errors <- learningCurve(cbind(1, train$X), train$y, cbind(1, val$X), val$y)
g2 <- plotLearningCurve(errors)
g2
# Chunk 13: polyFeatures
# Chunk 14: featureNormalize
# Chunk 15: polyPlots
# Chunk 16
polyPlots(train$X, train$y, val$X, val$y, n = 12, lambda = 0, p = 8)
# Chunk 17: validationCurve
source('~/coursera-ml-R/ex5/ex5_chunks.R')
source('~/coursera-ml-R/ex1/ex1_chunks.R')
# Chunk 1: dependencies
require(ggplot2)
# Chunk 2: read-data
train <- read.csv("../data/ex5train.csv")
val <- read.csv("../data/ex5val.csv")
test <- read.csv("../data/ex5test.csv")
# Chunk 3: viz-train
g1 <- ggplot(data = train, aes(X, y)) +
geom_point(shape = 4, color = "red", size = 3) +
labs(title = "Figure 1: Training Data",
x = "Change in water level (X)",
y = "Water flowing out of the dam (y)")
g1
# Chunk 4: ex1_chunks
knitr::read_chunk('../ex1/ex1_chunks.R')
# Chunk 5: lin-reg-cost-func
# Chunk 6: gradient-check
computeCost(cbind(1, train$X), train$y, c(1,1))
# Chunk 7: ex5_chunks
knitr::read_chunk('ex5_chunks.R')
# Chunk 8: trainLinearReg
# Chunk 9: optim
learnedTheta <- trainLinearReg(cbind(1, train$X), train$y)
# Chunk 10: plot-learnedTheta
g1 + geom_abline(slope = learnedTheta$par[2], intercept = learnedTheta$par[1])
lm(y ~ X + 1, train)$coefficients
learnedTheta$par
# Chunk 11: learningCurve
# Chunk 12: plot-learning-curves
errors <- learningCurve(cbind(1, train$X), train$y, cbind(1, val$X), val$y)
g2 <- plotLearningCurve(errors)
g2
# Chunk 13: polyFeatures
# Chunk 14: featureNormalize
# Chunk 15: polyPlots
# Chunk 16
polyPlots(train$X, train$y, val$X, val$y, n = 12, lambda = 0, p = 8)
# Chunk 17: validationCurve
# Chunk 18: test-validationCurve
lseq <- c(0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10)
errors <- validationCurve(polyFeatures(train$X, p = 8),
train$y,
polyFeatures(val$X, p = 8),
val$y,
lseq)
errors[, 1] <- 1:10 # for interpretability in the plot
plotLearningCurve(errors)
source('~/coursera-ml-R/ex5/ex5_chunks.R')
source('~/coursera-ml-R/ex1/ex1_chunks.R')
X= c(1, 1, 1)
X <- cbind(X, matrix(c(8, 1, 6, 3, 5, 7, 4, 9, 2), 3, 3, TRUE))
y = c(7, 6, 5)
theta = c(0.1, 0.2, 0.3, 0.4)
computeCost(X, y, theta, 0)
computeCost(X, y, theta, 7)
X = c(1, 2, 3, 4)
y = 5
theta = c(0.1, 0.2, 0.3, 0.4)
computeCost(X, y, theta, 7)
train <- read.csv("../data/ex5train.csv")
val <- read.csv("../data/ex5val.csv")
test <- read.csv("../data/ex5test.csv")
g1 <- ggplot(data = train, aes(X, y)) +
geom_point(shape = 4, color = "red", size = 3) +
labs(title = "Figure 1: Training Data",
x = "Change in water level (X)",
y = "Water flowing out of the dam (y)")
g1
knitr::read_chunk('../ex1/ex1_chunks.R')
computeCost(cbind(1, train$X), train$y, c(1,1))
debug(computeCost)
computeCost(cbind(1, train$X), train$y, c(1,1))
is.null(dim(X))
m
pred
cost
reg
setwd("ex5")
source("ex5_chunks.R")
source("../ex1/ex1_chunks.R")
# Chunk 1
source("ex5_chunks.R")
source("../ex1/ex1_chunks.R")
# Chunk 2: dependencies
require(ggplot2)
# Chunk 3: read-data
train <- read.csv("../data/ex5train.csv")
val <- read.csv("../data/ex5val.csv")
test <- read.csv("../data/ex5test.csv")
# Chunk 4: viz-train
g1 <- ggplot(data = train, aes(X, y)) +
geom_point(shape = 4, color = "red", size = 3) +
labs(title = "Figure 1: Training Data",
x = "Change in water level (X)",
y = "Water flowing out of the dam (y)")
g1
# Chunk 5: ex1_chunks
knitr::read_chunk('../ex1/ex1_chunks.R')
# Chunk 6: lin-reg-cost-func
# Chunk 7: gradient-check
computeCost(cbind(1, train$X), train$y, c(1,1))
# Chunk 8: ex5_chunks
knitr::read_chunk('ex5_chunks.R')
# Chunk 9: trainLinearReg
# Chunk 10: optim
learnedTheta <- trainLinearReg(cbind(1, train$X), train$y)
# Chunk 11: plot-learnedTheta
g1 + geom_abline(slope = learnedTheta$par[2], intercept = learnedTheta$par[1])
lm(y ~ X + 1, train)$coefficients
learnedTheta$par
# Chunk 12: learningCurve
# Chunk 13: plot-learning-curves
errors <- learningCurve(cbind(1, train$X), train$y, cbind(1, val$X), val$y)
g2 <- plotLearningCurve(errors)
g2
# Chunk 14: polyFeatures
# Chunk 15: featureNormalize
# Chunk 16: polyPlots
# Chunk 17
polyPlots(train$X, train$y, val$X, val$y, n = 12, lambda = 0, p = 8)
# Chunk 18: validationCurve
require(ggplot2)
computeCost(matrix(c(1, 2, 1, 3, 1, 4, 1, 5), 4, 2, TRUE),
c(7, 6, 5, 4),
c(0.1, 0.2))
# 11.9450
computeCost(matrix(c(1, 2, 3, 1, 3, 4, 1, 4, 5, 1, 5, 6), 4, 3, TRUE),
c(7,6,5,4),
c(0.1,0.2,0.3))
# 7.0175
X = matrix(c(2, 1, 3, 7, 1, 9, 1, 8, 1, 3, 7, 4), 4, 3, TRUE)
X
c(0.4, 0.8, 0.8)
computeCost(matrix(c(2, 1, 3, 7, 1, 9, 1, 8, 1, 3, 7, 4), 4, 3, TRUE),
c(2, 5, 5, 6),
c(0.4, 0.8, 0.8))
setwd("ex5")
source('~/coursera-ml-R/ex1/ex1_chunks.R', echo=TRUE)
source('~/coursera-ml-R/ex5/ex5_chunks.R', echo=TRUE)
